---
title: "Gender Gap Decomposition"
#author: "YOUR NAME"
date: "April 2021"
output: 
  html_document:
    highlight: haddock
    theme: journal
    number_sections: no
    toc: yes
    toc_depth: 2
    toc_float: yes
    code_folding: hide
---

<style>

table, td, th {
  border: none;
  padding-left: 1em;
  padding-right: 1em;
  min-width: 50%;
  margin-left: auto;
  margin-right: auto;
  margin-top: 1em;
  margin-bottom: 1em;
}

h1.title {
  font-size: 38px;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 18px;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  text-align: center;
}

</style>

  
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)
```

```{r load_tidyverse}
# Ensure that pacman is installed for package management and loading.
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse) # for data reading wrangling and visualization

```


```{r load_packages}
# for enabling dataframe manipulation
pacman::p_load(dplyr)
# for modeling, transforming, and visualizing data
pacman::p_load(tidyverse)
# for simplifying the process of creating tidy dat
pacman::p_load(tidyr)
# for working with tabular data
pacman::p_load(data.table)
pacman::p_load(devtools)
# for data visualization
pacman::p_load(ggplot2)
# for plotting correlations 
pacman::p_load(corrplot)  # Correlations (0.84)
# provides support to ggplot2 for labeling graphs
pacman::p_load(directlabels)
# for providing a general-purpose tool for dynamic report generation
pacman::p_load(knitr)
# for providing a prettier RMarkdown (1.0.1)
pacman::p_load(kableExtra)
pacman::p_load(stargazer)
options(scipen = 1000)
pacman::p_load('fastDummies')
pacman::p_load(Formula)
source_url("https://raw.githubusercontent.com/MatthieuStigler/Misconometrics/master/Gelbach_decompo/dec_covar.R")

```


# Introduction

In the previous tutorial, we found that women had larger returns to Year Up than men. In this tutorial, we want to know what drives this difference, and whether it can be explained by various factors, such as men's pre Year Up earnings or women's occupational choices. This is called a *decomposition*. There are many different decomposition methods. The one we use here is called Gelbach decomposition, after [Gelbach, (2016)](http://dx.doi.org/10.1086/683668).   

Think of the example of the gender wage gap, *i.e.* the difference in earnings between men and women. The unconditional difference in mean earnings between men and women is currently around 20 percent in the United States - men have 20 percent higher average earnings overall [(see Blau and Kahn, (2016), for a study on the gender wage gap)](http://www.nber.org/papers/w21913). 

However, around half of that, i.e. 10 percentage points in the difference, can be "explained" by other factors, such as women's labor force participation, working hours, occupational choice, family status or whether they have children. What we mean by "can be explained",  is that if we compare men and women with the same occupation, with the same employment status and working hours and the same family status - the raw gap diminishes to 10 percent. 

Those factors can "account" for the difference - because men and women have different distributions of employment, occupation and family status. On the other hand, that also means that there is a residual 10 percent difference in earnings between men and women that we cannot explain away with observable variables. This difference may be due to discrimination, or preferences or any other factor that we as researchers, do not directy observe in the data. 

The goal of a decomposition analysis, is to know how much of the explained difference can be attributed to which factor: of the 10 percentage points in explained variation of the gender pay gap, how much can be explained by differences in education, versus occupational choice, versus child bearing decisions?


## In this tutorial

In previous tutourials we have seen that mens' returns to Year Up are significantly smaller than womens'. Why is that? With this this tutorial, we want to be able understand where this difference is coming from. What drives womens' higher returns to Year Up compared to men? What does that mean for Year Up's admittance strategy and program design? Is it that Year Up works less well for their male participants or is it just that women make greater gains relative to men?

To answer these questions, we need to decompose the gender gap in program returns. To that end, we want to look at all the different components that go into the returns analysis (1) actual changes in wages, (2) counterfactual changes in wages, (3) the difference between the two. We use the first component of actual changes in wages as the example to lead you through a decomposition here. 

This tutorial consists of the following steps: \
1) We remind ourselves of the Year Up alumni sample and look at correlations between our variables. \
2) We run linear regressions of earnings changes on gender and other covariates. \
3) Using a Gelbach decomposition method, we decompose the gender gap in earning changes by comparing the unconditional difference to a full model. 
4) We explain the Gelbach decomposition more formally.\
5) We explore what the next steps need to be. \


**NOTE: Before running the code below, please make sure you have run the model-building tutorial as well as the model heterogeneity tutorials and have all the model objects saved in your project folder (i.e. the `vars.rds` object below).** 

```{r, echo = FALSE}
# load cleaned CPS data
save_folder <- "intermediate"

# cps<-readRDS("cols.rds") #the cps sample we built the model on 
yu_treatment_control <- readRDS(paste(save_folder, "yu_treatment_control.rds", sep="/")) #the constructed sample with alumni and counterfactuals
```

```{r,echo = FALSE}
# class(cps)<-class(as.data.frame(cps))
# categorical<-c("year","region_large","occ1digit")
# cps[ , categorical] <- lapply(cps[ ,categorical] , factor)
cps_vars<--readRDS(paste(save_folder, "vars.rds", sep="/"))
```


# Our Year Up sample

```{r,echo = FALSE}

yu_treatment_control<-yu_treatment_control %>%rename(male=sex)
yu_df<- yu_treatment_control %>% filter(treatment==1)


yu_df <- dummy_cols(yu_df, select_columns = 'occ1digit')

```


As in the previous tutorial, we start out with our sample of YearUp alumni, for whom we observe pre-and post YearUp (actual) wages. Let us recap which covariates we have and what the sample looks like. 

## Codebook 
| Variable name                           | Description                                    |
|-----------------------------------------|------------------------------------------------|
| log_d_earn                              | Difference in log earnings  at t+1 and t       |
| real_yearly_earnings                    | Earnings at t                                  |
| male                                    | Male                                           |
| race_white                              | Race: While                                    |
| race_asian                              | Race: Asian                                    |
| race_black                              | Race: Black                                    |
| edu_num                                 | Years of Education                             |
| experience                              | Years of Experience                            |
| experience2                             | Years of Experience Squared                    |
| age                                     | Age                                            |
| age2                                    | Age Squared                                    |
| year                                    | Year of YU entry                               |
| RTIa                                    | Previous occupation: task index                |
| task_abstract                           | Previous occupation: abstract task measure     |
| task_manual                             | Previous occupation: manual task measure       |
| task_routine                            | Previous occupation: routine task measure      |
| Occupation Groups (previous occupation) |                                                |
| occ1digit_art_sports_media              | Arts, Design, Entertainment, Sports, and Media |
| occ1digit_building_cleaning             | Building and Grounds Cleaning and Maintenance  |
| occ1digit_business_op                   | Business Operations Specialists                |
| occ1digit_community                     | Community and Social Services                  |
| occ1digit_computer_mathematica          | Computer and Mathematical                      |
| occ1digit_construction                  | Construction                                   |
| occ1digit_educ                          | Education, Training, and Library               |
| occ1digit_farm_fish_forest              | Farming, Fisheries, and Forestry               |
| occ1digit_finacial_spec                 | Financial Specialists                          |
| occ1digit_food_serving                  | Food Preparation and Serving                   |
| occ1digit_healthcare_supp               | Healthcare Support                             |
| occ1digit_healthcare_tech               | Healthcare Practitioners and Technicians       |
| occ1digit_install_maint_rep             | Installation, Maintenance, and Repair          |
| occ1digit_life_physical_ssc             | Life, Physical, and Social Science             |
| occ1digit_management                    | Management in Business, Science, and Arts      |
| occ1digit_offic_admin                   | Office and Administrative Support              |
| occ1digit_pers_care                     | Personal Care and Service                      |
| occ1digit_production                    | Production                                     |
| occ1digit_protective_serv               | Protective Service                             |
| occ1digit_sales                         | Sales and Related                              |
| occ1digit_technician                    | Technicians                                    |
| occ1digit_transport                     | Transportation and Material Moving             |
| Location characteristics                |                                                |
| cty_pop2000                             | County Population in 2000                      |
| cz_pop2000                              | Commuting Zone Population in 2000              |
| intersects_msa                          | Urban Area                                     |
| cs00_seg_inc                            | Income Segregation                             |
| cs00_seg_inc_pov25                      | Segregation of Poverty (< p25)                 |
| cs00_seg_inc_aff75                      | Segregation of Affluence (>p75)                |
| cs_race_theil_2000                      | Racial Segregation                             |
| gini99                                  | Gini Index Within Bottom 99%                   |
| poor_share                              | Poverty Rate                                   |
| inc_share_1perc                         | Top 1% Income Share                            |
| frac_middleclass                        | Fraction Middle Class (p25-p75)                |
| scap_ski90pcm                           | Social Capital Index                           |
| rel_tot                                 | Percent Religious                              |
| cs_frac_black                           | Percent Black                                  |
| cs_frac_hisp                            | Percent Hispanic                               |
| unemp_rate                              | Unemployment Rate in 2000                      |
| pop_d_2000_1980                         | Percent Change in Population 1980-2000         |
| lf_d_2000_1980                          | Percent Change in Labor Force 1980-2000        |
| cs_labforce                             | Labor Force Participation                      |
| cs_elf_ind_man                          | Share Working in Manufacturing                 |
| cs_born_foreign                         | Percent Foreign Born                           |
| mig_inflow                              | Migration Inflow Rate                          |
| mig_outflow                             | Migration Outflow Rate                         |
| pop_density                             | Population Density                             |
| frac_traveltime_lt15                    | Fraction with Commute < 15 Min                 |
| hhinc00                                 | Mean Household Income                          |
| median_house_value                      | Median House Value                             |
| ccd_exp_tot                             | School Expenditure per Student                 |
| ccd_pup_tch_ratio                       | Student-Teacher Ratio                          |
| score_r                                 | Test Score Percentile (Income Adjusted)        |
| dropout_r                               | High School Dropout Rate (Income Adjusted)     |
| cs_educ_ba                              | Percent College Grads                          |
| tuition                                 | College Tuition                                |
| gradrate_r                              | Percent College Grads                          |
| e_rank_b                                | Absolute Mobility (Expected Rank at p25)       |
| cs_fam_wkidsinglemom                    | Fraction of Children with Single Mother        |
| crime_total                             | Total Crime Rate                               |
| subcty_exp_pc                           | Local Government Expenditures                  |
| taxrate                                 | Local Tax Rate                                 |
| tax_st_diff_top20                       | Tax Progressivity                              |
| region_large                            | 4 Regions                                      |

## Summary Statistics
Please note that we are using the data that we scaled in the last tutorial here. 
```{r summary_stats,  message=FALSE, echo = TRUE}
# Make a data.frame containing summary statistics of interest

yu_dfsum<-yu_df %>% dplyr::select(log_d_earn, real_yearly_earnings.1_pre,  real_yearly_earnings.2_post, where(is.numeric), -c(PACE_ID,value,RTIa, state_fips, fips,treatment, log_real_yearly_earnings_1,cz,county_fips, metfips))

summ_stats <- fBasics::basicStats(yu_dfsum)
summ_stats <- as.data.frame(t(summ_stats))

# Rename some of the columns for convenience
summ_stats <- summ_stats[c("Mean", "Stdev", "Minimum", "1. Quartile", "Median",  "3. Quartile", "Maximum")]
colnames(summ_stats)[colnames(summ_stats) %in% c('1. Quartile', '3. Quartile')] <- c('Lower quartile', 'Upper quartile')
```

```{r summary_stats_table, message=FALSE, echo = TRUE}
# Pretty-printing in HTML
summ_stats_table <- kable(summ_stats, "html", digits = 2)
kable_styling(summ_stats_table,
              bootstrap_options=c("striped", "hover", "condensed", "responsive"),
              full_width=FALSE)
```

## Correlations
```{r,echo = TRUE }
outcome<- "log_d_earn"#"log_d_earn"#""

male<- c("male")#,#"log_real_yearly_earnings_1")

# cps <- dummy_cols(cps, select_columns = 'occ1digit')
occ_group1 <-yu_df %>%
  select(tidyselect::vars_select(colnames(cps_vars), starts_with('occ1digit_', ignore.case = TRUE)))
occupation<-colnames(occ_group1)
occupation<-occupation[occupation!="occ1digit_transport"] 

tasks<-c("task_abstract","task_routine","task_manual" )
background1<-c("age","race_white","race_black","race_asian","edu_num")

occexp<- c("experience",tasks)#,occupation)
            

region<-c("cty_pop2000","cz_pop2000","intersects_msa",
                    "cs00_seg_inc","cs00_seg_inc_pov25","cs00_seg_inc_aff75",
                    "cs_race_theil_2000","gini99","poor_share","inc_share_1perc",
                    "frac_middleclass","rel_tot","cs_frac_black",
                    "cs_frac_hisp","unemp_rate","pop_d_2000_1980","lf_d_2000_1980",
                    "cs_labforce","cs_elf_ind_man","cs_born_foreign","mig_inflow",
                    "mig_outflow","pop_density","frac_traveltime_lt15","hhinc00",
                    "median_house_value","cs_educ_ba",
                    "cs_fam_wkidsinglemom","crime_total","subcty_exp_pc",
                    "taxrate","tax_st_diff_top20")

```

When doing decompositions, as in this case a gender gap decomposition, it is useful to know how the variables in our data are correlated with one another. This helps to get a first idea of which variables may be influencing the gap and how. Presenting pairwise correlations is easy with the `corrplot` function from the `corrplot` package. On the table below, if the (unadjusted) p-value for a pair of variables is less than 0.05, its square is not colored. 

Note that here we are excluding the geography variables and occupation groups for better readability of the plot.

```{r cor plot, echo = TRUE, fig.width=5, fig.height=5, warning=FALSE}
# Note: if the plot looks too cramped, try increasing fig.width and fig.height in the line above
yu_dfsum<-yu_dfsum %>% dplyr::select(-c(starts_with('occ1digit_', ignore.case = TRUE),all_of(region)))
pairwise_pvalues <- psych::corr.test(yu_dfsum, yu_dfsum)$p
corrplot(cor(yu_dfsum),
         type="upper",
         tl.col="black",
         order="hclust",
         tl.cex=1,
         addgrid.col = "black",
         p.mat=pairwise_pvalues,
         sig.level=0.05,
         number.font=10,
         insig="blank")
```

# The gender difference in post YU earning changes
## Regression Analysis

**Outcome:  Difference in log earnings between t and t+1 for program particpants** \
Since we are interested in decomposing the gender gap in treatment outcomes (Year Up returns), which in itself is a composite product of actual wage changes and counterfactual wage changes, it is instructive to start with looking at the gap in these separates building blocks first.  Let's start with simple linear regressions on the difference in log earnings between t and t+1 for program particpants (actual observed wage changes of pre and post earnings). 

```{r,echo = TRUE}
outcome<- "log_d_earn"
lm1<- as.formula(
  paste(outcome,
        paste(c(male), collapse = " + "),
        sep = " ~ "))
model1<-lm(lm1, data = yu_df)

lm2<- as.formula(
  paste(outcome,
        paste(c(male,background1), collapse = " + "),
        sep = " ~ "))
model2<-lm(lm2, data = yu_df)

lm3<- as.formula(
  paste(outcome,
        paste(c(male,background1,occexp), collapse = " + "),
        sep = " ~ "))
model3<-lm(lm3, data = yu_df)

lm4<- as.formula(
  paste(outcome,
        paste(c(male,background1,occexp,region), collapse = " + "),
       sep = " ~ "))
model4<-lm(lm4,  data = yu_df)

lm5<- as.formula(
  paste(outcome,
        paste(c(male,background1,occexp,region, "log_real_yearly_earnings_1"), collapse = " + "),
       sep = " ~ "))
model5<-lm(lm5,  data = yu_df)
```

```{r,  echo = TRUE, results = 'asis'}
stargazer(model1, model2,model3, model4,model5,
          title="OLS regression of difference in log earnings between t and t+1 for program particpants, consecutively adding more covariates",
          type = 'html',
          omit=c(background1,occexp,region,"log_real_yearly_earnings_1"),
          df = FALSE,
          add.lines = list(c("Demographics", "No", "Yes", "Yes", "Yes","Yes"),c("Occupation+Experience", "No", "No","Yes","Yes", "Yes"),  c("Local labor market characteristics", "No", "No", "No", "Yes","Yes" ), c("Earnings at t", "No", "No", "No", "No","Yes")),
          notes.append = FALSE,
          notes = c("Dependent variable: change in log earnings.","Significance level: * p$<$0.10, ** p$<$0.05, *** p$<$0.01. For further notes, see text." )
)

```

The regression table shows results from 5 linear regressions, in each of which we keep adding more controls, which means that we add more variables beyond male to the regression.

- Column 1 includes just the male coefficient, Column 2 additionally includes age, race white, race asian, race black and years of schooling. Column 3 additionally to the coviarites in column 2 includes experience, occupation group indicators and task measures. Column 4 also includes YU site location labor market characteristics like population density,  local unemployment rates or average schooling levels. Column 5 finally additionally controls for pre-treatment earnings. 
<p>


Column 1  is the univariate regression of difference in log earnings on male, i.e. the unconditional gender wage change gap: 

$$log(\Delta y_{i})= \beta_1 Male + \epsilon{_i}$$

- The male coefficient tells us that overall on average, men have higher earnings changes compared to women.
Specifically, this unconditional gender difference on the difference in log earnings between t and t+1 for Year up candidates is 0.061 (to convert the coefficient to the log odds ratio (since our outcome is log(a)-log(b)=log(a/b), we can take the expoential and get: exp(0.042)=1.043), i.e. the ratio of male to female earning changes is 1.043. In other words men have 4.3% higher YOY earning changes before and after YU than females. \


- In columns 2 to 5, we consecutively add more variables to the regression, as denoted in the bottom rows of the table and explained in the notes above. 

$$log(\Delta y_{i})= \beta_1 Male + \sum_{k=1}^K\beta_k Z_{ik}+  \epsilon{_i}$$

- We do not report the coefficients to those other variables, because we are not interested in them, they are merely "controls".  "Controlling" for a covariate means you add it to a regression and thereby net out the effect this variable may have on the outcome. It is as if you were re-weighting the sample to have the same distribution with respect to the control variable. Here, we do not actually care what the effect of the control variable itself is. 

- In column 2 we add demographics such as the race, education and age of the individual to the regression.  This decreases the male coefficient to 0.034 - i.e. holding constant race, education and age, decreases the difference in earning changes for men to 3.4%. Or: if race, education and age were on average the same across men and women in our sample, the difference in gender for earnings changes would be 0.034.  

- What that means is that race, education and age are correlated with both male and the outcome, and ignoring those variables makes the gender gap larger that it should be (we "overestimate" the gender gap), while in truth it may be the case that men are older in our sample and older people have higher earning changes - and therefore some of the effect should be attributed to age rather than gender.  


- Column 5 shows that in the "the full model", controlling for everything we think should go into that regression, the gender gap is at 0.035, or 3.5% (=exp(0.035)) greater change in earnings for men. That is smaller than in column 1 but larger than in column 2.  Holding constant age, education, experience, race, prior occupation group and task measures within that occupation, location characteristics and previous wages, we reduce the unconditional gender gap. 


(Note: In this example, the extend to which we "explain" away the difference in earning changes in gender is small. When you do this for (log) yearly earnings in levels rather than changes, the gender gap and the difference covarariates make in explaining it, become more pronounced.  You should try it out!) 

# Gelbach Decomposition of Year Up wage changes

We next want to know, which covariate contributes how and how much, in reducing the unconditional gender difference in earnings changes.

Gelbach [(Gelbach, 2016)](http://dx.doi.org/10.1086/683668) proposes a decomposition method that compares a baseline model with just our coefficient of interest (here male), with a full model including all relevant variables and decomposes the contribution of all the variables to the changes in the coefficient of interest.

As we see in the regression table, controling for or including more variables in the regression, decreases the gender gap (it decreases the size of the positive and significant coefficient of male on earning changes), from 6.3% to 5.5% higher average earning changes for men compared to women after Year Up. 

<p>

```{r,  echo = TRUE, results = 'asis'}

dec_long_2 <- dec_covar(object = model5, var_main = c("male"), format = "long", add_coefs = TRUE, conf.int = TRUE)

# add occupation group
 dec_long_2a<-dec_long_2 %>%
   mutate(group= if_else(covariate %in% c("age", "age2"), "age",
                         if_else(covariate %in% c("race_white", "race_black", "race_asian"), "race",
                                if_else(covariate %in% c("experience", "experience2"), "experience",
                                        if_else(covariate %in% c("log_real_yearly_earnings_1"), "pre-earnings",
   if_else(covariate %in% occupation, "occupation_group",
           if_else(covariate %in% tasks, "occupation_tasks",
           if_else(covariate %in% region, "regional",   
                         "education"))))))))
# 
# 
sumthose<-  c("beta_K", "beta_K_low", "beta_K_high",
              "gamma","gamma_low", "gamma_high", "delta")

keepthose<- c("beta_var_base", "beta_var_full")

dec_long_2b <- dec_long_2a %>%
  group_by(group)%>%
  summarise_at(vars(all_of(sumthose)), funs(sum), na.rm=TRUE)%>%
  rename(covariate=group)

dec_long_2c <- dec_long_2a %>%
  group_by(group)%>%
  summarise_at(vars(all_of(keepthose)), funs(max), na.rm=TRUE)%>%
  rename(covariate=group)

dec_long_2d <-left_join(dec_long_2b,dec_long_2c,by="covariate")

dec_long_2d$variable<-c(rep("male", times=(dim(dec_long_2d)[1])))

plot_dec(dec_long_2d) +
   ggtitle("Effect of each covariate group on the male coef")
```


<p>
The plot depicts the differences in the male coefficients between column 1 and 5 in the regression table above.  

- Model base (black vertical line) represents the size of the male coefficient in the univariate regression of male difference in earnings changes, i.e. the unconditional gender wage change gap in column 1. 
- Model full (dotted blue vertical line)  represents the size of the male coefficient in the spefification in column 5, where we also control for (i.e. include as other covariates in the regression) a host of factors such as age, experience, race, prior occupation group, location characteristics and pre earnings. 

As noted above, the unconditional difference, i.e. the male coefficient in column 1 is 0.042 while the male coefficient in the full model is reduced to 0.035. This also means that we "explain" 0.007 log points of the gender gap through the extra controls. This is the gap between the black and blue line. 

How do the included covariates contribute to that change in the coefficient? To better read the results, we group covariates together (by summing over them). Here, we group the age terms, race indicators, years of education, experience, occupation grups, occupational tasks and all regional covariates together. 

The length of the horizontal arms going from the full model indicate the contribution of each variable (-group) to the male coefficient in the full model. It tells us what the male coefficient would be, if
(conditional on all the other variables in the model) the means of that variable were the same across males and females. That is, if we were to re-weight our sample such that it had equal means for men and women in the respective covariate the dot of that covariate arm would be the size of the male coefficient. 

- For example, all else constant, if the distribution of age were the same across both males and females in YU participants, the gender gap would be 0.009 log points smaller, so would be 0.033. That is, as we can see, actually smaller than the full model coefficient ends up being. We can also say that age explains 0.009 log points of the gender gap. The reason why the male coefficient ends up being larger than the respective race, age or location contributions, is because other covariates "pull" the male coefficient into the opposite direction. 

- Earnings at year t, has the opposite effect, meaning that controlling for that factor makes the gender gap larger: If males and females had the same pre-treatment earnings in our sample, the gap would be even larger at 7%.


## Gamma-beta plot

The result that certain covariates would increase the gender gap as opposed to reducing it, can be counter-intuitive. How come some variables have a positive and others a negative contribution to explaining the variation in the gender gap? To better understand where this result comes from, it is instructive to look at the two components that make up this "contribution"  $\delta$ (delta) here - $\gamma$ and $\beta$ (gamma and beta). (We explain this more formally below, too).  


- Beta denotes how the covariate is directly correlated with the outcome (e.g. how age is correlated with earnings changes), 
- While gamma denotes how the covariate is correlated with gender (how age is correlated with male).
- The size of the bubbles represent the absolute size of delta (how much age contributes in explaining the explainable gender gap). 

<p>
<center>
```{r, echo = TRUE}
dec_long_2<-dec_long_2 %>%
  filter(!(covariate %in% c(region, colnames(occ_group1))))

plot_gamma_beta(dec_long_2, add_CI = TRUE) +
  ggtitle("Covariate impact: direct (beta) and indirect (gamma) impact")
```
(For better visibility, we take out the regional covariates and occupation group dummies here.) 
</center>
<p>

Let's inspect a few covariates with different beta and gammas in turn, to understand what can drive the sign and size of delta. 

- Starting with age, we can see that both beta and gamma are positive, meaning that age is both positively correlated with earning changes (beta, direct effect) and with male (gamma, indirect effect). It means that older individuals have higher earning changes, and that the men in our sample are older than females. If males and females in our sample were the same age on average, that would reduce the gender gap. 

- When we do not control for age, the male-female gap in earning changes is larger than it actually is when we compare individuals of equal age. The larger raw difference can therefore be attributed to the difference in age between men and women in our sample and the fact that older individuals see greater changes (both needs to be true, otherwise the age difference in the sample would be irrelevant if age and earning were unrelated). 

- For pre-treatment earnings (log_real_yearly_earnings_1), gamma is positive, but beta is negative, meaning that men have higher pre-treatment earnings, but higher pre-treatment earnings are associated with lower earnings changes. This means that if the distribution of pre-treatment earnings was the same across males and females in our sample, the gender gap would be even larger. 

- For black, both beta and gamma are negative, meaning that black participants have smaller earning changes, and more blacks in our sample are female than male. If the share of blacks across males and females in our sample were the same, we would see a smaller gender gap. 

This shows that in order to understand what drives the decomposition pieces, we really need to  understand the covarariances between both the omitted variable and the outcome and the omitted variable and the coefficient (male here) in question.
<p>


# Gelbach decomposition formally

For those interested in the foundations, this section lays out the Gelbach decomposition more formally.

In a regression where we are interested in the effect of some variable X on Y, where we know that a third variable Z is also related to Y and X but where we do not include that Z, Z is an "omitted variable". Think of our above example, where male is our X, earning changes our Y and Z is age. When we omitted age from the regression, some of the effect of age on earnings was wrongly attributed to male, because male and age are positively correlated (as we saw in both the correlation plot, as well as the gamma-beta plot). In econometrics, we call this an "omitted variable bias", i.e. a biased estimate of some true effect, because you are not controlling for a variable you should be controlling for. 


Gelbach's decomposition, essentially decomposes the omitted variable bias from excluding other variables into their respective contributions. 

For illustration, take  the two regression equations below,  representing a full and a baseline model, respectively. Let's assume that the "missing" variables $Z_{i,s}$ in the base model are black and age. 

(1):
\begin{equation}
 ln(changeEarnings)_{sit}= \beta_1^{base}Male+ \epsilon_{sit}
\end{equation}


(2):	

\begin{equation}\label{eq:gel1}
	ln(changeEarnings)_{sit}= \beta_1^{full}Male+\beta_2Z_{i,s}+ \epsilon_{sit} 
\end{equation}


where (1) is the baseline model, (2) is the "full" model here. 

$\beta_1^{base}$ then includes both the "true" effect of male on earnings, as well as the omitted variable bias: 
	
	
\begin{equation}
\beta_1^{base}= \beta_1+\delta_{Male}= \beta_1+\Gamma\beta_2
\end{equation}

 The difference in $\beta_1^{base}$ and $\beta_1^{full}$ is exactly $\delta_{Male}$ - and $\delta_{Male}$ is the "omitted variable bias".  $\delta_{Male}$ is made up of $\Gamma\beta_2$ of all omitted variables. $\Gamma$ is the matrix of coefficients from projecting the columns of $Z_{i,s}$ on $Male$, i.e. here how male is correlated with age and black. So \Gamma comes from regressions of:
\begin{equation}
	Black= \Gamma_0^{Black} +Male\Gamma_{Male}^{Black}+ W
\end{equation}
\begin{equation}\label{eq:gel5}
	Age= \Gamma_0^{age} +Male\Gamma_{Male}^{age}+ W
\end{equation}

where W are residuals.
The overall $\delta_{Male}$ can hence be decomposed into the contributions of the different variables that are omitted:

\begin{equation}\label{eq:gel6}
\beta_1^{full}-\beta_1^{base} = \delta_{Male}=\underbrace{\beta_2^{Black}\Gamma_{Male}^{Black}}_{\text{Black contribution}}+\underbrace{\beta_2^{age}\Gamma_{Male}^{age}}_{\text{age contribution}}= \delta_{male}^{Black} + \delta_{male}^{age} 
\end{equation}
	
What this decomposition tells us is how much of the male gap is explained by variation in black and age, $\delta_{male}^{Black}$ and $\delta_{male}^{age}$ are their respective components of the explained part (the part that goes away after controlling for those covarariates). These "contributions" always have the same unit as the outcome - in our case we are decomposing the changes in log earnings- so the $\delta$s are can also be interpreted as their contribution in explaining the gender gap in log points.  

As we know from the analysis above (gamma-beta plot), each $\delta$ is a product of $\beta$ and $\gamma$ - the respective direct effect (how the covariates is correlated with the outcome) and indirect effect (how the covariate is correlated with the main covariate of interest we are decomposing).


One nice feature of the Gelbach decomposition is that the deltas are additive, which means you can group them together into meaninful groups and  look at their joint contribution. In our example below we e.g. group together the many location characteristic variables into one "region" group - we simply want to know how controling for regional characteristics in general impacts the gender gap, not each single one of them. This tremendously facilitates illustration, discussion and therefore usefulness of the decomposition. 

A word of caution and the reason why we do the Gelbach decomposition: after having seeing the regression table above, where we consecutively added more controls,  you may be tempted to say "Why can I not simply consecutively add each covariate to a regression and look at how the coefficient changes based on adding the extra control"? This used to indeed be common practice among empirical researchers. However, a main point of Gelbach's paper (you should take a look at the paper itself, you can find it in the course materials) was to show the flaw in such an approach. It turns out that it matters a lot in which order you add covariates to the model (e.g in the regression table above, we could have either first added location covariates in one regression and then add occuation covariates or the other way around). That is why Gelbach proposes to compare a full and a baseline model and properly decompose the contributions of all additional covariates to the coefficient in the base model. 

# Next steps

We went through the example of a decomposition for actual earning changes.  You should next try to do the same for the (1) counterfactual predicitons and (2) the difference in actual versus counterfactual earnings. This should help you understand what is driving the difference in treatment effects. 

Next, you will want to repeat this exercise for any subgroups you identified as important and interesting in the previous tutorial. 

